{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ba307a6",
   "metadata": {},
   "source": [
    "# tr, val, te 데이터들 각각 라벨별로 폴더에 이동시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fc11a678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "48224db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"train_humans = os.listdir('./DataSet/Train')\\nval_humans = os.listdir('./DataSet/Validation')\\ntest_humans = os.listdir('./DataSet/Test')\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''train_humans = os.listdir('./DataSet/Train')\n",
    "val_humans = os.listdir('./DataSet/Validation')\n",
    "test_humans = os.listdir('./DataSet/Test')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ae2a7c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv('./Labels/TrainLabels.csv')\n",
    "val_labels = pd.read_csv('./Labels/ValidationLabels.csv')\n",
    "test_labels = pd.read_csv('./Labels/TestLabels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0c9c98d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ClipID</th>\n",
       "      <th>Boredom</th>\n",
       "      <th>Engagement</th>\n",
       "      <th>Confusion</th>\n",
       "      <th>Frustration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1100011002.avi</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1100011003.avi</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1100011004.avi</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1100011005.avi</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1100011006.avi</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ClipID  Boredom  Engagement  Confusion  Frustration \n",
       "0  1100011002.avi        0           2          0             0\n",
       "1  1100011003.avi        0           2          0             0\n",
       "2  1100011004.avi        0           3          0             0\n",
       "3  1100011005.avi        0           3          0             0\n",
       "4  1100011006.avi        0           3          0             0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "93d46d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"train_labels[train_labels['ClipID']=='1100011002.avi']\""
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''train_labels[train_labels['ClipID']=='1100011002.avi']'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2e4db9cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"train_0 = list(train_labels.loc[train_labels['Engagement']==0, 'ClipID'])\\ntrain_1 = list(train_labels.loc[train_labels['Engagement']==1, 'ClipID'])\\ntrain_2 = list(train_labels.loc[train_labels['Engagement']==2, 'ClipID'])\\ntrain_3 = list(train_labels.loc[train_labels['Engagement']==3, 'ClipID'])\\n\\nval_0 = list(val_labels.loc[val_labels['Engagement']==0, 'ClipID'])\\nval_1 = list(val_labels.loc[val_labels['Engagement']==1, 'ClipID'])\\nval_2 = list(val_labels.loc[val_labels['Engagement']==2, 'ClipID'])\\nval_3 = list(val_labels.loc[val_labels['Engagement']==3, 'ClipID'])\\n\\ntest_0 = list(test_labels.loc[test_labels['Engagement']==0, 'ClipID'])\\ntest_1 = list(test_labels.loc[test_labels['Engagement']==1, 'ClipID'])\\ntest_2 = list(test_labels.loc[test_labels['Engagement']==2, 'ClipID'])\\ntest_3 = list(test_labels.loc[test_labels['Engagement']==3, 'ClipID'])\""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''train_0 = list(train_labels.loc[train_labels['Engagement']==0, 'ClipID'])\n",
    "train_1 = list(train_labels.loc[train_labels['Engagement']==1, 'ClipID'])\n",
    "train_2 = list(train_labels.loc[train_labels['Engagement']==2, 'ClipID'])\n",
    "train_3 = list(train_labels.loc[train_labels['Engagement']==3, 'ClipID'])\n",
    "\n",
    "val_0 = list(val_labels.loc[val_labels['Engagement']==0, 'ClipID'])\n",
    "val_1 = list(val_labels.loc[val_labels['Engagement']==1, 'ClipID'])\n",
    "val_2 = list(val_labels.loc[val_labels['Engagement']==2, 'ClipID'])\n",
    "val_3 = list(val_labels.loc[val_labels['Engagement']==3, 'ClipID'])\n",
    "\n",
    "test_0 = list(test_labels.loc[test_labels['Engagement']==0, 'ClipID'])\n",
    "test_1 = list(test_labels.loc[test_labels['Engagement']==1, 'ClipID'])\n",
    "test_2 = list(test_labels.loc[test_labels['Engagement']==2, 'ClipID'])\n",
    "test_3 = list(test_labels.loc[test_labels['Engagement']==3, 'ClipID'])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bbbbc4c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"train_videos = []\\nfor human in train_humans:\\n    videos = os.listdir('./DataSet/Train/'+human)\\n    for video in videos:\\n        train_videos.append(video)\\n        \\nval_videos = []\\nfor human in val_humans:\\n    videos = os.listdir('./DataSet/Validation/'+human)\\n    for video in videos:\\n        val_videos.append(video)\\n\\ntest_videos = []\\nfor human in test_humans:\\n    videos = os.listdir('./DataSet/Test/'+human)\\n    for video in videos:\\n        test_videos.append(video)\""
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''train_videos = []\n",
    "for human in train_humans:\n",
    "    videos = os.listdir('./DataSet/Train/'+human)\n",
    "    for video in videos:\n",
    "        train_videos.append(video)\n",
    "        \n",
    "val_videos = []\n",
    "for human in val_humans:\n",
    "    videos = os.listdir('./DataSet/Validation/'+human)\n",
    "    for video in videos:\n",
    "        val_videos.append(video)\n",
    "\n",
    "test_videos = []\n",
    "for human in test_humans:\n",
    "    videos = os.listdir('./DataSet/Test/'+human)\n",
    "    for video in videos:\n",
    "        test_videos.append(video)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "251c6f75",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 9) (2977796808.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[77], line 9\u001b[1;36m\u001b[0m\n\u001b[1;33m    return '3''''\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 9)\n"
     ]
    }
   ],
   "source": [
    "'''def eng(video_name):\n",
    "    if ((video_name in train_0) or (video_name in val_0) or (video_name in test_0)):\n",
    "        return '0'\n",
    "    elif ((video_name in train_1) or (video_name in val_1) or (video_name in test_1)):\n",
    "        return '1'\n",
    "    elif ((video_name in train_2) or (video_name in val_2) or (video_name in test_2)):\n",
    "        return '2'\n",
    "    else:\n",
    "        return '3''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "37574705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"os.listdir('./DataSet/Train/110001/1100011002')[0]\""
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''os.listdir('./DataSet/Train/110001/1100011002')[0]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "45ee3383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"eng('1100011002.avi')\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''eng('1100011002.avi')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5f1e938b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"for human in train_humans:\\n    videos = os.listdir('./DataSet/Train/'+human)\\n    for video in videos:\\n        video_name = os.listdir('./DataSet/Train/'+human+'/'+video)[0]\\n        eng_num = eng(video_name)\\n        destination_path = './train/'+eng_num+'/'+ video_name\\n        shutil.copyfile('./DataSet/Train/'+human+'/'+video+'/'+video_name, destination_path)\""
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for human in train_humans:\n",
    "    videos = os.listdir('./DataSet/Train/'+human)\n",
    "    for video in videos:\n",
    "        video_name = os.listdir('./DataSet/Train/'+human+'/'+video)[0]\n",
    "        eng_num = eng(video_name)\n",
    "        destination_path = './train/'+eng_num+'/'+ video_name\n",
    "        shutil.copyfile('./DataSet/Train/'+human+'/'+video+'/'+video_name, destination_path)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ce32a722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"for human in val_humans:\\n    videos = os.listdir('./DataSet/Validation/'+human)\\n    for video in videos:\\n        video_name = os.listdir('./DataSet/Validation/'+human+'/'+video)[0]\\n        eng_num = eng(video_name)\\n        destination_path = './val/'+eng_num+'/'+ video_name\\n        shutil.copyfile('./DataSet/Validation/'+human+'/'+video+'/'+video_name, destination_path)\""
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for human in val_humans:\n",
    "    videos = os.listdir('./DataSet/Validation/'+human)\n",
    "    for video in videos:\n",
    "        video_name = os.listdir('./DataSet/Validation/'+human+'/'+video)[0]\n",
    "        eng_num = eng(video_name)\n",
    "        destination_path = './val/'+eng_num+'/'+ video_name\n",
    "        shutil.copyfile('./DataSet/Validation/'+human+'/'+video+'/'+video_name, destination_path)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "738e8e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"for human in test_humans:\\n    videos = os.listdir('./DataSet/Test/'+human)\\n    for video in videos:\\n        video_name = os.listdir('./DataSet/Test/'+human+'/'+video)[0]\\n        eng_num = eng(video_name)\\n        destination_path = './test/'+eng_num+'/'+ video_name\\n        shutil.copyfile('./DataSet/Test/'+human+'/'+video+'/'+video_name, destination_path)\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for human in test_humans:\n",
    "    videos = os.listdir('./DataSet/Test/'+human)\n",
    "    for video in videos:\n",
    "        video_name = os.listdir('./DataSet/Test/'+human+'/'+video)[0]\n",
    "        eng_num = eng(video_name)\n",
    "        destination_path = './test/'+eng_num+'/'+ video_name\n",
    "        shutil.copyfile('./DataSet/Test/'+human+'/'+video+'/'+video_name, destination_path)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27f89c4",
   "metadata": {},
   "source": [
    "# 데이터 준비 (csv 파일로 먼저 만들어놓기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "80646a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "label_types = os.listdir('train')\n",
    "print(label_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "598bf318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                    name\n",
      "0     0  train/0/1100131017.avi\n",
      "1     0  train/0/1100152070.avi\n",
      "2     0  train/0/1100171004.avi\n",
      "3     0  train/0/1100171008.avi\n",
      "4     0  train/0/1100412018.avi\n"
     ]
    }
   ],
   "source": [
    "names = []\n",
    "\n",
    "for label in label_types:\n",
    "    all_names = os.listdir('train' + '/' + label)\n",
    "    \n",
    "    for name in all_names:\n",
    "        names.append((label, str('train' + '/' + label) + '/' + name))\n",
    "        \n",
    "df_train = pd.DataFrame(data=names, columns=['label', 'name'])\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9971ad40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    2618\n",
       "2    2616\n",
       "1     213\n",
       "0      34\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_train.loc[:, ['name', 'label']]\n",
    "df_train\n",
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "38c05ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"df_train.to_csv('train.csv')\""
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''df_train.to_csv('train.csv')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a78e0f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation\n",
    "names = []\n",
    "\n",
    "for label in label_types:\n",
    "    all_names = os.listdir('val' + '/' + label)\n",
    "    \n",
    "    for name in all_names:\n",
    "        names.append((label, str('val' + '/' + label) + '/' + name))\n",
    "        \n",
    "df_val = pd.DataFrame(data=names, columns=['label', 'name'])\n",
    "\n",
    "# test\n",
    "names = []\n",
    "\n",
    "for label in label_types:\n",
    "    all_names = os.listdir('test' + '/' + label)\n",
    "    \n",
    "    for name in all_names:\n",
    "        names.append((label, str('test' + '/' + label) + '/' + name))\n",
    "        \n",
    "df_test = pd.DataFrame(data=names, columns=['label', 'name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d0cbc448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>val/0/4000222032.avi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>val/0/4000301011.avi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>val/0/4000301028.avi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>val/0/4000301030.avi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>val/0/4000301042.avi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1715</th>\n",
       "      <td>3</td>\n",
       "      <td>val/3/8263820275.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1716</th>\n",
       "      <td>3</td>\n",
       "      <td>val/3/8263820277.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1717</th>\n",
       "      <td>3</td>\n",
       "      <td>val/3/8263820278.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1718</th>\n",
       "      <td>3</td>\n",
       "      <td>val/3/8263820279.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1719</th>\n",
       "      <td>3</td>\n",
       "      <td>val/3/826382028.mp4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1720 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                  name\n",
       "0        0  val/0/4000222032.avi\n",
       "1        0  val/0/4000301011.avi\n",
       "2        0  val/0/4000301028.avi\n",
       "3        0  val/0/4000301030.avi\n",
       "4        0  val/0/4000301042.avi\n",
       "...    ...                   ...\n",
       "1715     3  val/3/8263820275.mp4\n",
       "1716     3  val/3/8263820277.mp4\n",
       "1717     3  val/3/8263820278.mp4\n",
       "1718     3  val/3/8263820279.mp4\n",
       "1719     3   val/3/826382028.mp4\n",
       "\n",
       "[1720 rows x 2 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a9bae5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = df_val.loc[:, ['name', 'label']]\n",
    "df_test = df_test.loc[:, ['name', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bb835fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"df_val.to_csv('validation.csv')\\ndf_test.to_csv('test.csv')\""
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''df_val.to_csv('validation.csv')\n",
    "df_test.to_csv('test.csv')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1cbd94b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    2618\n",
       "2    2616\n",
       "1     213\n",
       "0      34\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c1b69ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_0 = df_train[df_train['label'] == '0']\n",
    "tr_1 = df_train[df_train['label'] == '1'].sample(100)\n",
    "tr_2 = df_train[df_train['label'] == '2'].sample(100)\n",
    "tr_3 = df_train[df_train['label'] == '3'].sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ebc8abd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([tr_0, tr_1, tr_2, tr_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3d7d40b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train/0/1100131017.avi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train/0/1100152070.avi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train/0/1100171004.avi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train/0/1100171008.avi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train/0/1100412018.avi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4421</th>\n",
       "      <td>train/3/3100682007.avi</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2945</th>\n",
       "      <td>train/3/1100012033.avi</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3629</th>\n",
       "      <td>train/3/2100531024.avi</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5287</th>\n",
       "      <td>train/3/4140810252.avi</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4114</th>\n",
       "      <td>train/3/2408460268.avi</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>334 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        name label\n",
       "0     train/0/1100131017.avi     0\n",
       "1     train/0/1100152070.avi     0\n",
       "2     train/0/1100171004.avi     0\n",
       "3     train/0/1100171008.avi     0\n",
       "4     train/0/1100412018.avi     0\n",
       "...                      ...   ...\n",
       "4421  train/3/3100682007.avi     3\n",
       "2945  train/3/1100012033.avi     3\n",
       "3629  train/3/2100531024.avi     3\n",
       "5287  train/3/4140810252.avi     3\n",
       "4114  train/3/2408460268.avi     3\n",
       "\n",
       "[334 rows x 2 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2660e61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    813\n",
       "3    741\n",
       "1    143\n",
       "0     23\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5612be8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_0 = df_val[df_val['label'] == '0']\n",
    "val_1 = df_val[df_val['label'] == '1'].sample(20)\n",
    "val_2 = df_val[df_val['label'] == '2'].sample(20)\n",
    "val_3 = df_val[df_val['label'] == '3'].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "559a5ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = pd.concat([val_0, val_1, val_2, val_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "aa381829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    896\n",
       "2    882\n",
       "1     84\n",
       "0      4\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8f140231",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_0 = df_test[df_test['label'] == '0']\n",
    "test_1 = df_test[df_test['label'] == '1'].sample(20)\n",
    "test_2 = df_test[df_test['label'] == '2'].sample(20)\n",
    "test_3 = df_test[df_test['label'] == '3'].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c5481e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.concat([test_0, test_1, test_2, test_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b8ca62",
   "metadata": {},
   "source": [
    "## Train 시키기 위한 모듈 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "485b5bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/tensorflow/docs\n",
      "  Cloning https://github.com/tensorflow/docs to c:\\users\\user\\appdata\\local\\temp\\pip-req-build-f3mevc76\n",
      "  Resolved https://github.com/tensorflow/docs to commit 8246c1cba26bb07d9c02c165d83936c5b50825ca\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: astor in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-docs==2023.12.6.69331) (0.8.1)\n",
      "Requirement already satisfied: absl-py in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-docs==2023.12.6.69331) (1.4.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-docs==2023.12.6.69331) (3.1.2)\n",
      "Requirement already satisfied: nbformat in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-docs==2023.12.6.69331) (5.7.0)\n",
      "Requirement already satisfied: protobuf>=3.12 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-docs==2023.12.6.69331) (3.20.3)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-docs==2023.12.6.69331) (6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2->tensorflow-docs==2023.12.6.69331) (2.1.1)\n",
      "Requirement already satisfied: fastjsonschema in c:\\users\\user\\anaconda3\\lib\\site-packages (from nbformat->tensorflow-docs==2023.12.6.69331) (2.16.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from nbformat->tensorflow-docs==2023.12.6.69331) (4.17.3)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\user\\anaconda3\\lib\\site-packages (from nbformat->tensorflow-docs==2023.12.6.69331) (5.3.0)\n",
      "Requirement already satisfied: traitlets>=5.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from nbformat->tensorflow-docs==2023.12.6.69331) (5.7.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat->tensorflow-docs==2023.12.6.69331) (22.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat->tensorflow-docs==2023.12.6.69331) (0.18.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jupyter-core->nbformat->tensorflow-docs==2023.12.6.69331) (2.5.2)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jupyter-core->nbformat->tensorflow-docs==2023.12.6.69331) (305.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/tensorflow/docs 'C:\\Users\\user\\AppData\\Local\\Temp\\pip-req-build-f3mevc76'\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/tensorflow/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "25dac465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imutils in c:\\users\\user\\anaconda3\\lib\\site-packages (0.5.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3897cf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_docs.vis import embed\n",
    "from tensorflow import keras\n",
    "from imutils import paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7d99ae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a48e84fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)])\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b0a71d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total videos for training : 334\n",
      "Total videos for validation : 83\n",
      "Total videos for testing : 64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total videos for training : {len(train)}\")\n",
    "print(f\"Total videos for validation : {len(val)}\")\n",
    "print(f\"Total videos for testing : {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "00e67b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "\n",
    "\n",
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "\n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f3d0ba",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "619fa013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_extractor():\n",
    "    feature_extractor = keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "\n",
    "feature_extractor = build_feature_extractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21b5277",
   "metadata": {},
   "source": [
    "# Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b0eb6b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3]], dtype=int64)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(train[\"label\"]))\n",
    "print(label_processor.get_vocabulary())\n",
    "\n",
    "labels = train[\"label\"].values\n",
    "labels = label_processor(labels[..., None]).numpy()\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c13d7f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "MAX_SEQ_LENGTH = 100\n",
    "NUM_FEATURES = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c1b80c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train/0/1100131017.avi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train/0/1100152070.avi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train/0/1100171004.avi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train/0/1100171008.avi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train/0/1100412018.avi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4421</th>\n",
       "      <td>train/3/3100682007.avi</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2945</th>\n",
       "      <td>train/3/1100012033.avi</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3629</th>\n",
       "      <td>train/3/2100531024.avi</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5287</th>\n",
       "      <td>train/3/4140810252.avi</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4114</th>\n",
       "      <td>train/3/2408460268.avi</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>334 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        name label\n",
       "0     train/0/1100131017.avi     0\n",
       "1     train/0/1100152070.avi     0\n",
       "2     train/0/1100171004.avi     0\n",
       "3     train/0/1100171008.avi     0\n",
       "4     train/0/1100412018.avi     0\n",
       "...                      ...   ...\n",
       "4421  train/3/3100682007.avi     3\n",
       "2945  train/3/1100012033.avi     3\n",
       "3629  train/3/2100531024.avi     3\n",
       "5287  train/3/4140810252.avi     3\n",
       "4114  train/3/2408460268.avi     3\n",
       "\n",
       "[334 rows x 2 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "86cbc29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame features in train set : (334, 100, 2048)\n",
      "Frame masks in train set : (334, 100)\n",
      "train_labels in train set : (334, 1)\n",
      "test_labels in test set : (64, 1)\n"
     ]
    }
   ],
   "source": [
    "def prepare_all_videos(df, root_dir):\n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"name\"].values.tolist()\n",
    "    \n",
    "    ##take all classlabels from train_df column named 'tag' and store in labels\n",
    "    labels = df[\"label\"].values\n",
    "    \n",
    "    #convert classlabels to label encoding\n",
    "    labels = label_processor(labels[..., None]).numpy()\n",
    "\n",
    "    # `frame_masks` and `frame_features` are what we will feed to our sequence model.\n",
    "    # `frame_masks` will contain a bunch of booleans denoting if a timestep is\n",
    "    # masked with padding or not.\n",
    "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\") # 145,20\n",
    "    frame_features = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\") #145,20,2048\n",
    "\n",
    "    # For each video.\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        # Gather all its frames and add a batch dimension.\n",
    "        frames = load_video(os.path.join(root_dir, path))\n",
    "        frames = frames[None, ...]\n",
    "\n",
    "        # Initialize placeholders to store the masks and features of the current video.\n",
    "        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "        temp_frame_features = np.zeros(\n",
    "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "        # Extract features from the frames of the current video.\n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            for j in range(length):\n",
    "                temp_frame_features[i, j, :] = feature_extractor.predict(\n",
    "                    batch[None, j, :]\n",
    "                )\n",
    "            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "        frame_features[idx,] = temp_frame_features.squeeze()\n",
    "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
    "\n",
    "    return (frame_features, frame_masks), labels\n",
    "\n",
    "\n",
    "train_data, train_labels = prepare_all_videos(train,\"train\")\n",
    "val_data, val_labels = prepare_all_videos(val, 'val')\n",
    "test_data, test_labels = prepare_all_videos(test, \"test\")\n",
    "\n",
    "print(f\"Frame features in train set : {train_data[0].shape}\")\n",
    "print(f\"Frame masks in train set : {train_data[1].shape}\")\n",
    "\n",
    "print(f\"train_labels in train set : {train_labels.shape}\")\n",
    "print(f\"test_labels in test set : {test_labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9373c3b7",
   "metadata": {},
   "source": [
    "## The Sequence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "659ca75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.3854 - accuracy: 0.2305\n",
      "Epoch 1: val_loss improved from inf to 1.38676, saving model to ./tmp\\video_classifier\n",
      "11/11 [==============================] - 11s 305ms/step - loss: 1.3854 - accuracy: 0.2305 - val_loss: 1.3868 - val_accuracy: 0.2410\n",
      "Epoch 2/30\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.3829 - accuracy: 0.2994\n",
      "Epoch 2: val_loss did not improve from 1.38676\n",
      "11/11 [==============================] - 1s 85ms/step - loss: 1.3829 - accuracy: 0.2994 - val_loss: 1.3872 - val_accuracy: 0.2410\n",
      "Epoch 3/30\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.3809 - accuracy: 0.2994\n",
      "Epoch 3: val_loss did not improve from 1.38676\n",
      "11/11 [==============================] - 1s 102ms/step - loss: 1.3809 - accuracy: 0.2994 - val_loss: 1.3878 - val_accuracy: 0.2410\n",
      "Epoch 4/30\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.3787 - accuracy: 0.2994\n",
      "Epoch 4: val_loss did not improve from 1.38676\n",
      "11/11 [==============================] - 1s 102ms/step - loss: 1.3787 - accuracy: 0.2994 - val_loss: 1.3883 - val_accuracy: 0.2410\n",
      "Epoch 5/30\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.3767 - accuracy: 0.2994\n",
      "Epoch 5: val_loss did not improve from 1.38676\n",
      "11/11 [==============================] - 1s 91ms/step - loss: 1.3767 - accuracy: 0.2994 - val_loss: 1.3889 - val_accuracy: 0.2410\n",
      "Epoch 6/30\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.3748 - accuracy: 0.2994\n",
      "Epoch 6: val_loss did not improve from 1.38676\n",
      "11/11 [==============================] - 1s 94ms/step - loss: 1.3748 - accuracy: 0.2994 - val_loss: 1.3896 - val_accuracy: 0.2410\n",
      "Epoch 7/30\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.3727 - accuracy: 0.2994\n",
      "Epoch 7: val_loss did not improve from 1.38676\n",
      "11/11 [==============================] - 1s 88ms/step - loss: 1.3727 - accuracy: 0.2994 - val_loss: 1.3902 - val_accuracy: 0.2410\n",
      "Epoch 8/30\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.3710 - accuracy: 0.2994\n",
      "Epoch 8: val_loss did not improve from 1.38676\n",
      "11/11 [==============================] - 1s 86ms/step - loss: 1.3710 - accuracy: 0.2994 - val_loss: 1.3909 - val_accuracy: 0.2410\n",
      "Epoch 9/30\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.3691 - accuracy: 0.2994\n",
      "Epoch 9: val_loss did not improve from 1.38676\n",
      "11/11 [==============================] - 1s 83ms/step - loss: 1.3691 - accuracy: 0.2994 - val_loss: 1.3916 - val_accuracy: 0.2410\n",
      "Epoch 10/30\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.3673 - accuracy: 0.2994\n",
      "Epoch 10: val_loss did not improve from 1.38676\n",
      "11/11 [==============================] - 1s 83ms/step - loss: 1.3673 - accuracy: 0.2994 - val_loss: 1.3923 - val_accuracy: 0.2410\n",
      "Epoch 11/30\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.3657 - accuracy: 0.2994\n",
      "Epoch 11: val_loss did not improve from 1.38676\n",
      "11/11 [==============================] - 1s 91ms/step - loss: 1.3657 - accuracy: 0.2994 - val_loss: 1.3930 - val_accuracy: 0.2410\n",
      "Epoch 12/30\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.3641 - accuracy: 0.2994\n",
      "Epoch 12: val_loss did not improve from 1.38676\n",
      "11/11 [==============================] - 1s 87ms/step - loss: 1.3641 - accuracy: 0.2994 - val_loss: 1.3938 - val_accuracy: 0.2410\n",
      "Epoch 13/30\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.3626 - accuracy: 0.2994\n",
      "Epoch 13: val_loss did not improve from 1.38676\n",
      "11/11 [==============================] - 1s 85ms/step - loss: 1.3626 - accuracy: 0.2994 - val_loss: 1.3946 - val_accuracy: 0.2410\n",
      "Epoch 14/30\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.3609 - accuracy: 0.2994\n",
      "Epoch 14: val_loss did not improve from 1.38676\n",
      "11/11 [==============================] - 1s 89ms/step - loss: 1.3609 - accuracy: 0.2994 - val_loss: 1.3955 - val_accuracy: 0.2410\n",
      "Epoch 15/30\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.3594 - accuracy: 0.2994\n",
      "Epoch 15: val_loss did not improve from 1.38676\n",
      "11/11 [==============================] - 1s 92ms/step - loss: 1.3594 - accuracy: 0.2994 - val_loss: 1.3963 - val_accuracy: 0.2410\n",
      "Epoch 16/30\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.3580 - accuracy: 0.2994\n",
      "Epoch 16: val_loss did not improve from 1.38676\n",
      "11/11 [==============================] - 1s 94ms/step - loss: 1.3580 - accuracy: 0.2994 - val_loss: 1.3972 - val_accuracy: 0.2410\n",
      "Epoch 17/30\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.3565 - accuracy: 0.2994\n",
      "Epoch 17: val_loss did not improve from 1.38676\n",
      "11/11 [==============================] - 1s 88ms/step - loss: 1.3565 - accuracy: 0.2994 - val_loss: 1.3981 - val_accuracy: 0.2410\n",
      "Epoch 18/30\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.3552 - accuracy: 0.2994\n",
      "Epoch 18: val_loss did not improve from 1.38676\n",
      "11/11 [==============================] - 1s 89ms/step - loss: 1.3552 - accuracy: 0.2994 - val_loss: 1.3989 - val_accuracy: 0.2410\n",
      "Epoch 19/30\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.3539 - accuracy: 0.2994\n",
      "Epoch 19: val_loss did not improve from 1.38676\n",
      "11/11 [==============================] - 1s 90ms/step - loss: 1.3539 - accuracy: 0.2994 - val_loss: 1.3998 - val_accuracy: 0.2410\n",
      "Epoch 20/30\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.3527 - accuracy: 0.2994\n",
      "Epoch 20: val_loss did not improve from 1.38676\n",
      "11/11 [==============================] - 1s 99ms/step - loss: 1.3527 - accuracy: 0.2994 - val_loss: 1.4008 - val_accuracy: 0.2410\n",
      "Epoch 21/30\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.3515 - accuracy: 0.2994\n",
      "Epoch 21: val_loss did not improve from 1.38676\n",
      "11/11 [==============================] - 1s 101ms/step - loss: 1.3515 - accuracy: 0.2994 - val_loss: 1.4017 - val_accuracy: 0.2410\n",
      "Epoch 22/30\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.3504 - accuracy: 0.2994\n",
      "Epoch 22: val_loss did not improve from 1.38676\n",
      "11/11 [==============================] - 1s 98ms/step - loss: 1.3504 - accuracy: 0.2994 - val_loss: 1.4027 - val_accuracy: 0.2410\n",
      "Epoch 23/30\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.3492 - accuracy: 0.2994\n",
      "Epoch 23: val_loss did not improve from 1.38676\n",
      "11/11 [==============================] - 1s 96ms/step - loss: 1.3492 - accuracy: 0.2994 - val_loss: 1.4036 - val_accuracy: 0.2410\n",
      "Epoch 24/30\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.3481 - accuracy: 0.2994\n",
      "Epoch 24: val_loss did not improve from 1.38676\n",
      "11/11 [==============================] - 1s 93ms/step - loss: 1.3481 - accuracy: 0.2994 - val_loss: 1.4044 - val_accuracy: 0.2410\n",
      "Epoch 25/30\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.3471 - accuracy: 0.2994\n",
      "Epoch 25: val_loss did not improve from 1.38676\n",
      "11/11 [==============================] - 1s 95ms/step - loss: 1.3471 - accuracy: 0.2994 - val_loss: 1.4054 - val_accuracy: 0.2410\n",
      "Epoch 26/30\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.3461 - accuracy: 0.2994\n",
      "Epoch 26: val_loss did not improve from 1.38676\n",
      "11/11 [==============================] - 1s 89ms/step - loss: 1.3461 - accuracy: 0.2994 - val_loss: 1.4063 - val_accuracy: 0.2410\n",
      "Epoch 27/30\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.3451 - accuracy: 0.2994\n",
      "Epoch 27: val_loss did not improve from 1.38676\n",
      "11/11 [==============================] - 1s 90ms/step - loss: 1.3451 - accuracy: 0.2994 - val_loss: 1.4073 - val_accuracy: 0.2410\n",
      "Epoch 28/30\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.3441 - accuracy: 0.2994\n",
      "Epoch 28: val_loss did not improve from 1.38676\n",
      "11/11 [==============================] - 1s 97ms/step - loss: 1.3441 - accuracy: 0.2994 - val_loss: 1.4084 - val_accuracy: 0.2410\n",
      "Epoch 29/30\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.3431 - accuracy: 0.2994\n",
      "Epoch 29: val_loss did not improve from 1.38676\n",
      "11/11 [==============================] - 1s 99ms/step - loss: 1.3431 - accuracy: 0.2994 - val_loss: 1.4094 - val_accuracy: 0.2410\n",
      "Epoch 30/30\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.3423 - accuracy: 0.2695\n",
      "Epoch 30: val_loss did not improve from 1.38676\n",
      "11/11 [==============================] - 1s 93ms/step - loss: 1.3423 - accuracy: 0.2695 - val_loss: 1.4105 - val_accuracy: 0.2410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 31ms/step - loss: 1.3833 - accuracy: 0.3125\n",
      "Test accuracy: 31.25%\n"
     ]
    }
   ],
   "source": [
    "def get_sequence_model():\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "\n",
    "    # Refer to the following tutorial to understand the significance of using `mask`:\n",
    "    # https://keras.io/api/layers/recurrent_layers/gru/\n",
    "    x = keras.layers.GRU(16, return_sequences=True)(frame_features_input, mask=mask_input)\n",
    "    x = keras.layers.GRU(8)(x)\n",
    "    x = keras.layers.Dropout(0.4)(x)\n",
    "    x = keras.layers.Dense(8, activation=\"relu\")(x)\n",
    "    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n",
    "\n",
    "    rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
    "\n",
    "    rnn_model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return rnn_model\n",
    "\n",
    "EPOCHS = 30\n",
    "# Utility for running experiments.\n",
    "def run_experiment():\n",
    "    filepath = \"./tmp/video_classifier\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
    "    )\n",
    "\n",
    "    seq_model = get_sequence_model()\n",
    "    history = seq_model.fit(\n",
    "        [train_data[0], train_data[1]],\n",
    "        train_labels,\n",
    "        validation_data=([val_data[0], val_data[1]], val_labels),\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[checkpoint],\n",
    "    )\n",
    "\n",
    "    seq_model.load_weights(filepath)\n",
    "    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history, seq_model\n",
    "\n",
    "\n",
    "_, sequence_model = run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e0c5913e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test video path: test/2/5100091066.avi\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "  1: 25.14%\n",
      "  3: 25.12%\n",
      "  2: 25.04%\n",
      "  0: 24.70%\n"
     ]
    }
   ],
   "source": [
    "def prepare_single_video(frames):\n",
    "    frames = frames[None, ...]\n",
    "    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "\n",
    "    for i, batch in enumerate(frames):\n",
    "        video_length = batch.shape[0]\n",
    "        length = min(MAX_SEQ_LENGTH, video_length)\n",
    "        for j in range(length):\n",
    "            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
    "        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "    return frame_features, frame_mask\n",
    "\n",
    "\n",
    "def sequence_prediction(path):\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    frames = load_video(os.path.join(\"test\", path))\n",
    "    frame_features, frame_mask = prepare_single_video(frames)\n",
    "    probabilities = sequence_model.predict([frame_features, frame_mask])[0]\n",
    "\n",
    "    for i in np.argsort(probabilities)[::-1]:\n",
    "        print(f\"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n",
    "    return frames\n",
    "\n",
    "test_video = np.random.choice(test[\"name\"].values.tolist())\n",
    "print(f\"Test video path: {test_video}\")\n",
    "\n",
    "test_frames = sequence_prediction(test_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65eeb0c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa13f012",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
